{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16a1df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修正文件: D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py\n",
      "train.py 文件中的 imsave 已被修正。\n"
     ]
    }
   ],
   "source": [
    "# 构造 train.py 的完整路径\n",
    "train_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\train.py\"\n",
    "\n",
    "print(f\"修正文件: {train_py_path}\")\n",
    "with open(train_py_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 执行替换\n",
    "content = content.replace(\"from scipy.misc import imsave\", \"import imageio\")\n",
    "content = content.replace(\"imsave(\", \"imageio.imwrite(\")\n",
    "\n",
    "# 将修改后的内容写回文件\n",
    "with open(train_py_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"train.py 文件中的 imsave 已被修正。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10856cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在修正文件: D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py\n",
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py 文件中的 create_criterion 方法已被修正。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\train.py\"\n",
    "\n",
    "print(f\"正在修正文件: {train_py_path}\")\n",
    "\n",
    "# --- 读取原始文件内容 ---\n",
    "try:\n",
    "    with open(train_py_path, 'r', encoding='utf-8') as f:\n",
    "        original_code = f.read()\n",
    "\n",
    "    # --- 定义原始的有问题的函数体和修正后的函数体 ---\n",
    "    original_function_str = \"\"\"def create_criterion(self):\n",
    "        # w is for gan\n",
    "        if self.opts['gan'] == 'lsgan':\n",
    "            self.adv_criterion = lambda p,t,w: torch.mean((p-t)**2)  # sigmoid is applied here\n",
    "        elif self.opts['gan'] == 'wgan_gp':\n",
    "            self.adv_criterion = lambda p,t,w: (-2*t+1) * torch.mean(p)\n",
    "        elif self.opts['gan'] == 'gan':\n",
    "            self.adv_criterion = lambda p,t,w: -w*(torch.mean(t*torch.log(p+1e-8)) + torch.mean((1-t)*torch.log(1-p+1e-8)))\n",
    "        else:\n",
    "            raise ValueError('Invalid/Unsupported GAN: %s.' % self.opts['gan'])\"\"\"\n",
    "\n",
    "    corrected_function_str = \"\"\"def create_criterion(self):\n",
    "        # w is for gan\n",
    "        if self.opts['gan'] == 'lsgan':\n",
    "            # 修正: 将布尔值 t 显式转换为浮点数 float(t)\n",
    "            self.adv_criterion = lambda p,t,w: torch.mean((p-float(t))**2)  # sigmoid is applied here\n",
    "        elif self.opts['gan'] == 'wgan_gp':\n",
    "            # 修正: 将布尔值 t 显式转换为浮点数 float(t)\n",
    "            self.adv_criterion = lambda p,t,w: (-2*float(t)+1) * torch.mean(p)\n",
    "        elif self.opts['gan'] == 'gan':\n",
    "            # 修正: 将布尔值 t 显式转换为浮点数 float(t)\n",
    "            self.adv_criterion = lambda p,t,w: -w*(torch.mean(float(t)*torch.log(p+1e-8)) + torch.mean((1-float(t))*torch.log(1-p+1e-8)))\n",
    "        else:\n",
    "            raise ValueError('Invalid/Unsupported GAN: %s.' % self.opts['gan'])\"\"\"\n",
    "    \n",
    "    # --- 执行替换 ---\n",
    "    if original_function_str in original_code:\n",
    "        final_code = original_code.replace(original_function_str, corrected_function_str)\n",
    "        \n",
    "        # --- 将修改后的内容写回文件 ---\n",
    "        with open(train_py_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_code)\n",
    "        \n",
    "        print(f\"{train_py_path} 文件中的 create_criterion 方法已被修正。\")\n",
    "    else:\n",
    "        print(\"未在文件中找到需要修正的 create_criterion 函数，可能它已经被修改过了。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5351941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在修正文件: D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py\n",
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py 文件中的 _get_data 方法已被修正。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "train_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\train.py\"\n",
    "\n",
    "print(f\"正在修正文件: {train_py_path}\")\n",
    "\n",
    "try:\n",
    "    with open(train_py_path, 'r', encoding='utf-8') as f:\n",
    "        original_code = f.read()\n",
    "\n",
    "    # --- 定义原始的有问题的函数体和修正后的函数体 ---\n",
    "    # 使用正则表达式以应对可能存在的格式差异\n",
    "    old_function_pattern = re.compile(\n",
    "        r\"def _get_data\\(self, d\\):\\s*\\n\\s*return d\\.data\\[0\\] if isinstance\\(d, Variable\\) else d\"\n",
    "    )\n",
    "\n",
    "    # 修正后的函数更健壮、更现代化\n",
    "    corrected_function_str = \"\"\"def _get_data(self, d):\n",
    "        if isinstance(d, torch.Tensor):\n",
    "            return d.item()\n",
    "        return d\"\"\"\n",
    "\n",
    "    # --- 执行替换 ---\n",
    "    if old_function_pattern.search(original_code):\n",
    "        final_code = old_function_pattern.sub(corrected_function_str, original_code)\n",
    "        \n",
    "        # --- 将修改后的内容写回文件 ---\n",
    "        with open(train_py_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_code)\n",
    "        \n",
    "        print(f\"{train_py_path} 文件中的 _get_data 方法已被修正。\")\n",
    "    else:\n",
    "        print(\"未在文件中找到需要修正的 _get_data 函数，可能它已经被修改过了。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba82c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在修正文件: D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py\n",
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py 文件中的 sample 方法已被修正。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "print(f\"正在修正文件: {train_py_path}\")\n",
    "\n",
    "try:\n",
    "    with open(train_py_path, 'r', encoding='utf-8') as f:\n",
    "        original_code = f.read()\n",
    "\n",
    "    # --- 定义原始的有问题的函数体和修正后的函数体 ---\n",
    "    original_function_str = \"\"\"def sample(self):\n",
    "        batch_size = self.z.size(0)\n",
    "        n_row = self.rows_map[batch_size]\n",
    "        n_col = int(np.ceil(batch_size / float(n_row)))\n",
    "        samples = []\n",
    "        i = j = 0\n",
    "        for row in range(n_row):\n",
    "            one_row = []\n",
    "            # fake\n",
    "            for col in range(n_col):\n",
    "                one_row.append(self.fake[i].cpu().data.numpy())\n",
    "                i += 1\n",
    "            # real\n",
    "            for col in range(n_col):\n",
    "                one_row.append(self.real[j].cpu().data.numpy())\n",
    "                j += 1\n",
    "            samples += [np.concatenate(one_row, axis=2)]\n",
    "        samples = np.concatenate(samples, axis=1).transpose([1, 2, 0])\n",
    "\n",
    "        half = samples.shape[1] // 2\n",
    "        samples[:, :half, :] = samples[:, :half, :] - np.min(samples[:, :half, :])\n",
    "        samples[:, :half, :] = samples[:, :half, :] / np.max(samples[:, :half, :])\n",
    "        samples[:, half:, :] = samples[:, half:, :] - np.min(samples[:, half:, :])\n",
    "        samples[:, half:, :] = samples[:, half:, :] / np.max(samples[:, half:, :])\n",
    "        return samples\"\"\"\n",
    "\n",
    "    corrected_function_str = \"\"\"def sample(self):\n",
    "        batch_size = self.z.size(0)\n",
    "        n_row = self.rows_map.get(batch_size, int(np.sqrt(batch_size))) # Fallback for safety\n",
    "        n_col = int(np.ceil(batch_size / float(n_row)))\n",
    "        samples = []\n",
    "        i = j = 0\n",
    "        \n",
    "        # Detach tensors from the computation graph before converting to numpy\n",
    "        fake_samples = self.fake.cpu().detach().numpy()\n",
    "        real_samples = self.real.cpu().detach().numpy()\n",
    "\n",
    "        for row in range(n_row):\n",
    "            one_row = []\n",
    "            # fake\n",
    "            for col in range(n_col):\n",
    "                if i < fake_samples.shape[0]:\n",
    "                    one_row.append(fake_samples[i])\n",
    "                    i += 1\n",
    "            # real\n",
    "            for col in range(n_col):\n",
    "                if j < real_samples.shape[0]:\n",
    "                    one_row.append(real_samples[j])\n",
    "                    j += 1\n",
    "            samples += [np.concatenate(one_row, axis=2)]\n",
    "        samples = np.concatenate(samples, axis=1).transpose([1, 2, 0])\n",
    "\n",
    "        # Denormalize from [-1, 1] to [0, 1]\n",
    "        samples = (samples + 1) / 2.0\n",
    "        \n",
    "        # 修正: 将最终的numpy数组转换为 [0, 255] 范围的 uint8 类型\n",
    "        # 这是图片保存函数最能接受的、最标准的格式\n",
    "        samples = (samples * 255).clip(0, 255).astype(np.uint8)\n",
    "        return samples\"\"\"\n",
    "\n",
    "    # --- 执行替换 ---\n",
    "    if original_function_str in original_code:\n",
    "        final_code = original_code.replace(original_function_str, corrected_function_str)\n",
    "        \n",
    "        # --- 将修改后的内容写回文件 ---\n",
    "        with open(train_py_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_code)\n",
    "        \n",
    "        print(f\"{train_py_path} 文件中的 sample 方法已被修正。\")\n",
    "    else:\n",
    "        print(\"未在文件中找到需要修正的 sample 函数，可能它已经被修改过了。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f1ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在修正文件: D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py\n",
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\train.py 文件中的 compute_noise_strength 方法已被修正。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"正在修正文件: {train_py_path}\")\n",
    "\n",
    "try:\n",
    "    with open(train_py_path, 'r', encoding='utf-8') as f:\n",
    "        original_code = f.read()\n",
    "\n",
    "    # --- 定义存在问题的代码片段和修正后的片段 ---\n",
    "    old_line_snippet = \"np.clip(torch.mean(self.d_real).data[0], 0.0, 1.0)\"\n",
    "    corrected_line_snippet = \"np.clip(torch.mean(self.d_real).item(), 0.0, 1.0)\"\n",
    "\n",
    "    # --- 执行替换 ---\n",
    "    if old_line_snippet in original_code:\n",
    "        final_code = original_code.replace(old_line_snippet, corrected_line_snippet)\n",
    "        \n",
    "        # --- 将修改后的内容写回文件 ---\n",
    "        with open(train_py_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_code)\n",
    "        \n",
    "        print(f\"{train_py_path} 文件中的 compute_noise_strength 方法已被修正。\")\n",
    "    else:\n",
    "        print(\"未在文件中找到需要修正的代码行，可能它已经被修改过了。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29d011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\utils\\data.py 文件已被修正并更新。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\utils\\\\data.py\"\n",
    "\n",
    "# --- data.py 的完整修正版内容 ---\n",
    "corrected_data_py_code = r\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np \n",
    "import h5py\n",
    "import imageio              # 修正: 导入 imageio 用于保存图片\n",
    "from PIL import Image       # 修正: 导入 Pillow(PIL) 用于读取和缩放图片\n",
    "\n",
    "\n",
    "prefix = './datasets/'\n",
    "\n",
    "# 这个函数虽然在当前训练流程中未被激活的CelebA类使用，\n",
    "# 但我们还是为它修正了兼容性问题，以防未来使用。\n",
    "def get_img(img_path, is_crop=True, crop_h=256, resize_h=64, normalize=False):\n",
    "    # 修正: 使用 Pillow 读取图片\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    resize_w = resize_h\n",
    "    if is_crop:\n",
    "        crop_w = crop_h\n",
    "        w, h = img.size\n",
    "        j = int(round((h - crop_h) / 2.))\n",
    "        i = int(round((w - crop_w) / 2.))\n",
    "        img = img.crop((i, j, i + crop_w, j + crop_h))\n",
    "    \n",
    "    # 修正: 使用 Pillow 缩放图片，LANCZOS 是高质量的下采样滤镜\n",
    "    img = img.resize((resize_w, resize_h), Image.LANCZOS)\n",
    "    \n",
    "    cropped_image = np.array(img).astype(np.float32)\n",
    "\n",
    "    if normalize:\n",
    "        cropped_image = cropped_image / 127.5 - 1.0\n",
    "    \n",
    "    return np.transpose(cropped_image, [2, 0, 1])\n",
    "\n",
    "\n",
    "class CelebA():\n",
    "    def __init__(self):\n",
    "        # 注意: .h5 文件名在这里被硬编码了\n",
    "        # 请确保你用 h5tool.py 创建的文件名与此一致\n",
    "        datapath = 'celeba-128x128.h5'\n",
    "        \n",
    "        self._base_key = 'data'\n",
    "        h5_path = os.path.join(prefix, datapath)\n",
    "        \n",
    "        if not os.path.exists(h5_path):\n",
    "            raise FileNotFoundError(f\"数据集文件未找到: {h5_path}。请先运行 h5tool.py 生成数据集。\")\n",
    "\n",
    "        self.dataset = h5py.File(h5_path, 'r')\n",
    "        self._len = {k:len(self.dataset[k]) for k in self.dataset.keys() if k.startswith('data')}\n",
    "        \n",
    "        # 检查所需的分辨率是否存在\n",
    "        # 例如，对于128x128的目标，我们需要4x4到128x128的所有层级\n",
    "        max_resol = 0\n",
    "        for k in self._len.keys():\n",
    "            res = int(k.replace('data','').split('x')[0])\n",
    "            if res > max_resol:\n",
    "                max_resol = res\n",
    "        \n",
    "        required_resolutions = [f'data{2**r}x{2**r}' for r in range(2, int(np.log2(max_resol)) + 1)]\n",
    "        for r_str in required_resolutions:\n",
    "             if r_str not in self.dataset:\n",
    "                 raise KeyError(f\"所需的分辨率 '{r_str}' 在HDF5文件中未找到。可用分辨率: {list(self.dataset.keys())}\")\n",
    "\n",
    "\n",
    "    def __call__(self, batch_size, size, level=None):\n",
    "        key = self._base_key + '{}x{}'.format(size, size)\n",
    "        idx = np.random.randint(self._len[key], size=batch_size)\n",
    "        batch_x = np.array([self.dataset[key][i] for i in idx], dtype=np.float32)\n",
    "        batch_x = batch_x / 127.5 - 1.0 # 归一化到 [-1, 1]\n",
    "        \n",
    "        if level is not None and level != int(level):\n",
    "            # 这是PGGAN核心的 \"fade-in\" 逻辑\n",
    "            alpha = level - int(level)\n",
    "            \n",
    "            # 从下一个更低的分辨率进行上采样\n",
    "            if size > 4: # 最小分辨率是 4x4\n",
    "                lr_key = self._base_key + '{}x{}'.format(size//2, size//2)\n",
    "                low_resol_batch_x_lr = np.array([self.dataset[lr_key][i] for i in idx], dtype=np.float32)\n",
    "                low_resol_batch_x_lr = low_resol_batch_x_lr / 127.5 - 1.0\n",
    "                \n",
    "                # 使用 repeat 进行简单的最近邻上采样\n",
    "                low_resol_batch_x = low_resol_batch_x_lr.repeat(2, axis=2).repeat(2, axis=3)\n",
    "                \n",
    "                # 按alpha值混合高分辨率和上采样后的低分辨率图像\n",
    "                batch_x = batch_x * alpha + low_resol_batch_x * (1 - alpha)\n",
    "                \n",
    "        return batch_x\n",
    "\n",
    "    def save_imgs(self, samples, file_name):\n",
    "        N_samples, channel, height, width = samples.shape\n",
    "        N_row = N_col = int(np.ceil(N_samples**0.5))\n",
    "        \n",
    "        # 将样本从 [-1, 1] 范围转换回 [0, 255] 的图片范围\n",
    "        samples = ((samples + 1.0) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "        \n",
    "        combined_imgs = np.ones((channel, N_row*height, N_col*width), dtype=np.uint8)\n",
    "        for i in range(N_row):\n",
    "            for j in range(N_col):\n",
    "                if i*N_col+j < samples.shape[0]:\n",
    "                    combined_imgs[:,i*height:(i+1)*height, j*width:(j+1)*width] = samples[i*N_col+j]\n",
    "        \n",
    "        combined_imgs = np.transpose(combined_imgs, [1, 2, 0])\n",
    "        \n",
    "        # 修正: 使用 imageio.imwrite 来替代过时的 imsave\n",
    "        imageio.imwrite(file_name + '.png', combined_imgs)\n",
    "\n",
    "\n",
    "class RandomNoiseGenerator():\n",
    "    def __init__(self, size, noise_type='gaussian'):\n",
    "        self.size = size\n",
    "        self.noise_type = noise_type.lower()\n",
    "        assert self.noise_type in ['gaussian', 'uniform']\n",
    "        if self.noise_type == 'gaussian':\n",
    "            self.generator = lambda s: np.random.randn(*s)\n",
    "        elif self.noise_type == 'uniform':\n",
    "            self.generator = lambda s: np.random.uniform(-1, 1, size=s)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.generator([batch_size, self.size]).astype(np.float32)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 将修正后的完整代码写入 `data.py` 文件\n",
    "try:\n",
    "    with open(data_py_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(corrected_data_py_code)\n",
    "    print(f\"{data_py_path} 文件已被修正并更新。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"找不到路径 {data_py_path}。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77ec922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\utils\\logger.py 文件已被一个更新的版本覆盖。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "logger_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\utils\\\\logger.py\"\n",
    "\n",
    "# --- logger.py 的完整修正版内容 ---\n",
    "corrected_logger_py_code = r\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "# A new, modern logger for PyTorch using the standard TensorBoard library.\n",
    "# This version removes the unnecessary TensorFlow 1.x dependency.\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 不再需要 tensorflow 或 scipy.misc\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, log_dir):\n",
    "        # Create a summary writer logging to log_dir.\n",
    "        # 使用 PyTorch 自带的 SummaryWriter\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        # Log a scalar variable.\n",
    "        # 使用新的 API: add_scalar\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \n",
    "        # Log a list of images.\n",
    "        # The PyTorch SummaryWriter add_images method expects a tensor \n",
    "        # in the format (N, C, H, W).\n",
    "        \n",
    "        # (此函数在当前 train.py 中未被调用，但我们提供一个正确的实现)\n",
    "        if isinstance(images, list):\n",
    "            images = np.array(images)\n",
    "\n",
    "        # 如果图像格式是 (N, H, W, C)，转换为 (N, C, H, W)\n",
    "        if images.ndim == 4 and images.shape[3] in [1, 3]:\n",
    "            images = images.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # PyTorch 的 add_images 可以直接处理 NumPy 数组或 PyTorch Tensor\n",
    "        self.writer.add_images(tag, images, step, dataformats='NCHW')\n",
    "\n",
    "    def histo_summary(self, tag, values, step, bins='auto'):\n",
    "        # Log a histogram of the tensor of values.\n",
    "        # 使用新的 API: add_histogram\n",
    "        self.writer.add_histogram(tag, values, step, bins=bins)\n",
    "        self.writer.flush() # 保持和原版行为一致，在记录后立即刷新\n",
    "\n",
    "    def close(self):\n",
    "        # Close the writer.\n",
    "        self.writer.close()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 将修正后的完整代码写入 `logger.py` 文件\n",
    "try:\n",
    "    with open(logger_py_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(corrected_logger_py_code)\n",
    "    print(f\"{logger_py_path} 文件已被一个更新的版本覆盖。\")\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")\n",
    "    print(f\"请确认路径 '{logger_py_path}' 是正确的。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e828e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\models\\base_model.py 文件已被修正并更新。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_model_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\models\\\\base_model.py\"\n",
    "\n",
    "\n",
    "# --- 这是 base_model.py 的完整修正版内容 ---\n",
    "corrected_base_model_py_code = r\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "# 修正: 导入 kaiming_normal_ (带下划线)\n",
    "from torch.nn.init import kaiming_normal_, calculate_gain\n",
    "import numpy as np\n",
    "import sys\n",
    "if sys.version_info.major == 3:\n",
    "    from functools import reduce\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "class PixelNormLayer(nn.Module):\n",
    "\n",
    "    # Pixelwise feature vector normalization.\n",
    "\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(PixelNormLayer, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(eps = %s)' % (self.eps)\n",
    "\n",
    "\n",
    "class WScaleLayer(nn.Module):\n",
    "\n",
    "    # Applies equalized learning rate to the preceding layer.\n",
    "\n",
    "    def __init__(self, incoming):\n",
    "        super(WScaleLayer, self).__init__()\n",
    "        self.incoming = incoming\n",
    "        self.scale = (torch.mean(self.incoming.weight.data ** 2)) ** 0.5\n",
    "        self.incoming.weight.data.copy_(self.incoming.weight.data / self.scale)\n",
    "        self.bias = None\n",
    "        if self.incoming.bias is not None:\n",
    "            self.bias = self.incoming.bias\n",
    "            self.incoming.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.scale * x\n",
    "        if self.bias is not None:\n",
    "            x += self.bias.view(1, self.bias.size()[0], 1, 1)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        param_str = '(incoming = %s)' % (self.incoming.__class__.__name__)\n",
    "        return self.__class__.__name__ + param_str\n",
    "\n",
    "\n",
    "def mean(tensor, axis, **kwargs):\n",
    "    if isinstance(axis, int):\n",
    "        axis = [axis]\n",
    "    for ax in sorted(axis, reverse=True):\n",
    "        tensor = torch.mean(tensor, dim=ax, **kwargs)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class MinibatchStatConcatLayer(nn.Module):\n",
    "    # Minibatch stat concatenation layer.\n",
    "    def __init__(self, averaging='all'):\n",
    "        super(MinibatchStatConcatLayer, self).__init__()\n",
    "        self.averaging = averaging.lower()\n",
    "        if 'group' in self.averaging:\n",
    "            self.n = int(self.averaging[5:])\n",
    "        else:\n",
    "            assert self.averaging in ['all', 'flat', 'spatial', 'none', 'gpool'], 'Invalid averaging mode'%self.averaging\n",
    "        self.adjusted_std = lambda x, **kwargs: torch.sqrt(torch.mean((x - torch.mean(x, **kwargs)) ** 2, **kwargs) + 1e-8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = list(x.size())\n",
    "        target_shape = shape.copy()\n",
    "        vals = self.adjusted_std(x, dim=0, keepdim=True)\n",
    "        if self.averaging == 'all':\n",
    "            target_shape[1] = 1\n",
    "            vals = torch.mean(vals, dim=1, keepdim=True)\n",
    "        elif self.averaging == 'spatial':\n",
    "            if len(shape) == 4:\n",
    "                vals = mean(vals, axis=[2,3], keepdim=True)\n",
    "        elif self.averaging == 'none':\n",
    "            target_shape = [target_shape[0]] + [s for s in target_shape[1:]]\n",
    "        elif self.averaging == 'gpool':\n",
    "            if len(shape) == 4:\n",
    "                vals = mean(x, [0,2,3], keepdim=True)\n",
    "        elif self.averaging == 'flat':\n",
    "            target_shape[1] = 1\n",
    "            vals = torch.FloatTensor([self.adjusted_std(x)])\n",
    "        else:\n",
    "            target_shape[1] = self.n\n",
    "            vals = vals.view(self.n, self.shape[1]/self.n, self.shape[2], self.shape[3])\n",
    "            vals = mean(vals, axis=0, keepdim=True).view(1, self.n, 1, 1)\n",
    "        vals = vals.expand(*target_shape)\n",
    "        return torch.cat([x, vals], 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(averaging = %s)' % (self.averaging)\n",
    "\n",
    "\n",
    "class GDropLayer(nn.Module):\n",
    "    # Generalized dropout layer.\n",
    "    def __init__(self, mode='mul', strength=0.2, axes=(0,1), normalize=False):\n",
    "        super(GDropLayer, self).__init__()\n",
    "        self.mode = mode.lower()\n",
    "        assert self.mode in ['mul', 'drop', 'prop'], 'Invalid GDropLayer mode'%mode\n",
    "        self.strength = strength\n",
    "        self.axes = [axes] if isinstance(axes, int) else list(axes)\n",
    "        self.normalize = normalize\n",
    "        self.gain = None\n",
    "\n",
    "    def forward(self, x, deterministic=False):\n",
    "        if deterministic or not self.strength:\n",
    "            return x\n",
    "\n",
    "        rnd_shape = [s if axis in self.axes else 1 for axis, s in enumerate(x.size())]\n",
    "        if self.mode == 'drop':\n",
    "            p = 1 - self.strength\n",
    "            rnd = np.random.binomial(1, p=p, size=rnd_shape) / p\n",
    "        elif self.mode == 'mul':\n",
    "            rnd = (1 + self.strength) ** np.random.normal(size=rnd_shape)\n",
    "        else:\n",
    "            coef = self.strength * x.size(1) ** 0.5\n",
    "            rnd = np.random.normal(size=rnd_shape) * coef + 1\n",
    "\n",
    "        if self.normalize:\n",
    "            rnd = rnd / np.linalg.norm(rnd, keepdims=True)\n",
    "        \n",
    "        # 修正: 不再需要 Variable() 封装\n",
    "        rnd = torch.from_numpy(rnd).type(x.data.type())\n",
    "        if x.is_cuda:\n",
    "            rnd = rnd.cuda()\n",
    "        return x * rnd\n",
    "\n",
    "    def __repr__(self):\n",
    "        param_str = '(mode = %s, strength = %s, axes = %s, normalize = %s)' % (self.mode, self.strength, self.axes, self.normalize)\n",
    "        return self.__class__.__name__ + param_str\n",
    "\n",
    "\n",
    "def resize_activations(v, so):\n",
    "    # Resize activation tensor 'v' of shape 'si' to match shape 'so'.\n",
    "    si = list(v.size())\n",
    "    so = list(so)\n",
    "    assert len(si) == len(so) and si[0] == so[0]\n",
    "\n",
    "    # Decrease feature maps.\n",
    "    if si[1] > so[1]:\n",
    "        v = v[:, :so[1]]\n",
    "\n",
    "    # Shrink spatial axes.\n",
    "    if len(si) == 4 and (si[2] > so[2] or si[3] > so[3]):\n",
    "        assert si[2] % so[2] == 0 and si[3] % so[3] == 0\n",
    "        ks = (si[2] // so[2], si[3] // so[3])\n",
    "        v = F.avg_pool2d(v, kernel_size=ks, stride=ks, ceil_mode=False, padding=0)\n",
    "\n",
    "    # 修正: 使用 F.interpolate 替代已废弃的 F.upsample\n",
    "    if si[2] < so[2]: \n",
    "        assert so[2] % si[2] == 0 and so[2] // si[2] == so[3] // si[3]\n",
    "        v = F.interpolate(v, scale_factor=so[2]//si[2], mode='nearest')\n",
    "\n",
    "    # Increase feature maps.\n",
    "    if si[1] < so[1]:\n",
    "        z = torch.zeros((v.shape[0], so[1] - si[1]) + so[2:], device=v.device)\n",
    "        v = torch.cat([v, z], 1)\n",
    "    return v\n",
    "\n",
    "\n",
    "class GSelectLayer(nn.Module):\n",
    "    def __init__(self, pre, chain, post):\n",
    "        super(GSelectLayer, self).__init__()\n",
    "        assert len(chain) == len(post)\n",
    "        self.pre = pre\n",
    "        self.chain = chain\n",
    "        self.post = post\n",
    "        self.N = len(self.chain)\n",
    "\n",
    "    def forward(self, x, y=None, cur_level=None, insert_y_at=None):\n",
    "        if cur_level is None:\n",
    "            cur_level = self.N\n",
    "        \n",
    "        min_level, max_level = int(np.floor(cur_level-1)), int(np.ceil(cur_level-1))\n",
    "        alpha = cur_level - int(cur_level)\n",
    "        \n",
    "        if self.pre is not None:\n",
    "            x = self.pre(x)\n",
    "\n",
    "        if max_level < 0:\n",
    "            max_level = 0\n",
    "            \n",
    "        x = self.chain[0](x)\n",
    "        \n",
    "        if max_level == 0:\n",
    "            return self.post[0](x)\n",
    "\n",
    "        for level in range(1, max_level + 1):\n",
    "            x = self.chain[level](x)\n",
    "\n",
    "        out_max_level = self.post[max_level](x)\n",
    "        \n",
    "        if alpha > 0:\n",
    "            out_min_level = self.post[max_level - 1](self.chain[max_level-1].last_conv_fmap)\n",
    "            out = resize_activations(out_min_level, out_max_level.size()) * (1-alpha) + out_max_level * alpha\n",
    "        else:\n",
    "            out = out_max_level\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DSelectLayer(nn.Module):\n",
    "    def __init__(self, pre, chain, inputs):\n",
    "        super(DSelectLayer, self).__init__()\n",
    "        assert len(chain) == len(inputs)\n",
    "        self.pre = pre\n",
    "        self.chain = chain\n",
    "        self.inputs = inputs\n",
    "        self.N = len(self.chain)\n",
    "\n",
    "    def forward(self, x, y=None, cur_level=None, insert_y_at=None):\n",
    "        if cur_level is None:\n",
    "            cur_level = self.N\n",
    "\n",
    "        max_level, min_level = int(np.floor(self.N-cur_level)), int(np.ceil(self.N-cur_level))\n",
    "        alpha = cur_level - int(cur_level)\n",
    "        \n",
    "        if self.pre is not None:\n",
    "            x = self.pre(x)\n",
    "\n",
    "        out = {}\n",
    "        if max_level == min_level or alpha == 0:\n",
    "             x = self.inputs[max_level](x)\n",
    "             x = self.chain[max_level](x)\n",
    "        else:\n",
    "            out_max = self.inputs[max_level](x)\n",
    "            out_max = self.chain[max_level](out_max)\n",
    "\n",
    "            out_min = self.inputs[min_level](x)\n",
    "            \n",
    "            x = resize_activations(out_min, out_max.size()) * (1-alpha) + out_max * alpha\n",
    "            x = self.chain[min_level](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return torch.cat([x, y], 1)\n",
    "\n",
    "\n",
    "class ReshapeLayer(nn.Module):\n",
    "    def __init__(self, new_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.new_shape = new_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert reduce(lambda u,v: u*v, self.new_shape) == reduce(lambda u,v: u*v, x.size()[1:])\n",
    "        return x.view(-1, *self.new_shape)\n",
    "\n",
    "\n",
    "def he_init(layer, nonlinearity='conv2d', param=None):\n",
    "    nonlinearity = nonlinearity.lower()\n",
    "    if nonlinearity not in ['linear', 'conv1d', 'conv2d', 'conv3d', 'relu', 'leaky_relu', 'sigmoid', 'tanh']:\n",
    "        if not hasattr(layer, 'gain') or layer.gain is None:\n",
    "            gain = 0\n",
    "        else:\n",
    "            gain = layer.gain\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        assert param is not None, 'Negative_slope(param) should be given.'\n",
    "        gain = calculate_gain(nonlinearity, param)\n",
    "    else:\n",
    "        gain = calculate_gain(nonlinearity)\n",
    "    # 修正: 使用 kaiming_normal_ (带下划线)\n",
    "    kaiming_normal_(layer.weight, a=gain)\n",
    "\"\"\"\n",
    "\n",
    "# 将修正后的完整代码写入 `base_model.py` 文件\n",
    "try:\n",
    "    with open(base_model_py_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(corrected_base_model_py_code)\n",
    "    print(f\"{base_model_py_path} 文件已被修正并更新。\")\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea6d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\PGGAN\\PyTorch-progressive_growing_of_gans_on_CelebA\\debug.py 文件已被修正并更新。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 构造 debug.py 的完整路径\n",
    "debug_py_path = \"D:\\\\Code\\\\PGGAN\\\\PyTorch-progressive_growing_of_gans_on_CelebA\\\\debug.py\"\n",
    "\n",
    "# --- 读取原始文件内容 ---\n",
    "try:\n",
    "    with open(debug_py_path, 'r', encoding='utf-8') as f:\n",
    "        original_code = f.read()\n",
    "    \n",
    "    # --- 执行所有必要的替换 ---\n",
    "    # 1. 移除过时的 Variable 封装\n",
    "    fixed_code = original_code.replace(\"from torch.autograd import Variable\\n\", \"\")\n",
    "    fixed_code = fixed_code.replace(\"Variable(\", \"\") # 注意这里会把括号也一并去掉\n",
    "    \n",
    "    # 2. 修正 .cuda(1) 为 .cuda()\n",
    "    fixed_code = fixed_code.replace(\".cuda(1)\", \".cuda()\")\n",
    "\n",
    "    # 3. 修正 .data[0] 为 .item()\n",
    "    fixed_code = fixed_code.replace(\".data[0]\", \".item()\")\n",
    "    \n",
    "    # --- 将修正后的完整代码写入 `debug.py` 文件 ---\n",
    "    with open(debug_py_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(fixed_code)\n",
    "    print(f\"{debug_py_path} 文件已被修正并更新。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
